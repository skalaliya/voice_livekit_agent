# LiveKit Voice AI Agents - Production Ready

Build robust, real-time voice AI assistants powered by local Ollama models, Deepgram speech-to-text, Silero VAD, and OpenAI TTS. This repo includes multiple specialized agents plus a **Universal Agent** that can handle ANY type of question.

## üöÄ Available Agents

### 1. **Universal Agent** (NEW ‚≠ê - Recommended)
**The most robust and versatile agent that can handle ANY question across all domains.**

Features:
- ‚úÖ Comprehensive error handling and graceful degradation
- ‚úÖ Dynamic tool registration (auto-enables based on API keys)
- ‚úÖ Persistent conversation memory and context tracking
- ‚úÖ Multi-domain knowledge (science, tech, math, history, travel, health, finance, etc.)
- ‚úÖ Built-in calculator, unit converter, dictionary, weather, news
- ‚úÖ Web search capabilities (optional)
- ‚úÖ Code execution in sandbox (optional)

```bash
# Quick start with Universal Agent
uv run python -m voice_livekit_agent.universal_agent console
```

üìñ **[Full Documentation](docs/UNIVERSAL_AGENT.md)**

### 2. Basic Agent
Simple demo agent with Airbnb booking capabilities. Great for learning the basics.

```bash
uv run python -m voice_livekit_agent.livekit_basic_agent console
```

### 3. French Voice Tutor
Interactive French language learning with CEFR levels and roleplay scenarios.

```bash
uv run python -m voice_livekit_agent.french_voice_tutor console
```

### 4. French Tutor Plus
Advanced version with spaced-repetition quizzes, pronunciation practice, and progress tracking.

```bash
uv run python -m voice_livekit_agent.french_voice_tutor_plus console
```

### 5. MCP Agent
Agent with Model Context Protocol integration for extended capabilities.

```bash
uv run python -m voice_livekit_agent.livekit_mcp_agent console
```

## üéØ Which Agent Should I Use?

- **Universal Agent** ‚≠ê - Best for: production apps, handling any question, robust error handling
- **Basic Agent** - Best for: learning, simple demos, Airbnb bookings
- **French Tutor** - Best for: language learning, CEFR practice
- **French Tutor Plus** - Best for: advanced learning, spaced repetition
- **MCP Agent** - Best for: Model Context Protocol integration

üëâ **New users**: Start with the [Universal Agent Quick Start](docs/QUICKSTART.md)

## Quickstart (copy, paste, run)
```bash
# 1) Model (local LLM via Ollama)
ollama pull llama3.2:3b
ollama serve   # keep running in another tab

# 2) Install Python deps (uv prefers pyproject.toml)
uv sync
# or, if you want the short form:
uv add "livekit-agents[openai]~=1.2" livekit-plugins-deepgram livekit-plugins-silero python-dotenv requests

# 3) .env (safe placeholders only)
cat > .env <<'EOF'
OPENAI_BASE_URL=http://127.0.0.1:11434/v1
OPENAI_API_KEY=ollama
LLM_MODEL=llama3.2:3b
LLM_TEMPERATURE=0.6
BASIC_TEST=true
WARMUP_LLM=true
WARMUP_TIMEOUT=60
# Voice mode
# DEEPGRAM_API_KEY=YOUR_DEEPGRAM_KEY
# OPENAI_TTS_API_KEY=YOUR_TTS_KEY
# TTS_VOICE=alloy
# USE_TOOLS=true
EOF
```

```bash
# Smoke test (LLM only path)
uv run python -m voice_livekit_agent.livekit_basic_agent console

# French tutor basics
uv run python -m voice_livekit_agent.french_voice_tutor console

# French tutor PLUS (quizzes, due words, pronunciation, exports)
uv run python -m voice_livekit_agent.french_voice_tutor_plus console

# Daily reminder script (CSV + macOS notification)
uv run python scripts/srs_daily.py
```

## Architecture deep-dive
```mermaid
flowchart LR
  classDef mic fill:#1e90ff,stroke:#0a3d62,color:#fff
  classDef vad fill:#c56cf0,stroke:#5f27cd,color:#fff
  classDef stt fill:#10ac84,stroke:#006266,color:#fff
  classDef agent fill:#feca57,stroke:#ee5253,color:#222
  classDef llm fill:#ff9f43,stroke:#d35400,color:#222
  classDef tts fill:#54a0ff,stroke:#1e3799,color:#fff
  classDef spk fill:#2ecc71,stroke:#145a32,color:#fff

  Mic:::mic --> VAD:::vad --> STT:::stt --> Agent:::agent --> LLM:::llm --> TTS:::tts --> Speaker:::spk
```

```mermaid
sequenceDiagram
  participant U as User (Mic)
  participant V as VAD (Silero)
  participant S as STT (Deepgram)
  participant A as LiveKit Agent
  participant L as LLM (Ollama OpenAI-compat)
  participant T as TTS (OpenAI TTS)
  U->>V: Raw audio frames
  V->>S: Speech segments only
  S-->>A: Partial & final transcripts (streaming)
  A->>L: Prompt + context (+ tools if enabled)
  L-->>A: Tokens (streaming)
  A->>T: Speak this text
  T-->>U: PCM audio chunks
```

## How it works under the hood (for curious teens)
- **VAD (Silero)** snips silence so Deepgram only hears real speech, cutting costs and latency.
- **Streaming STT (Deepgram Nova-2)** feeds partial transcripts to the agent; the console shows words as they land.
- **LiveKit Agent loop** glues STT, LLM, TTS, and `@function_tool`s together. When `USE_TOOLS=true`, methods like `due_words`, `start_quiz`, or `check_pronunciation` become callable from model responses.
- **LLM via Ollama** hits `http://127.0.0.1:11434/v1` through OpenAI-compatible APIs. `_warmup_ollama()` sends a tiny prompt so your first real reply doesn‚Äôt stall.
- **TTS (OpenAI)** streams MP3 chunks back. Short sentences give smoother speech, so replies stay snappy.
- **Latency tips:** keep VAD thresholds default, pick lighter models (`llama3.2:1b`) when you‚Äôre in a rush, and keep responses under two sentences.
- **Persistence:** tutor progress lives in `french_progress_plus.json` (see `SRS_STATE_FILE`); exports land in `~/.french_tutor/exports`.
- **Spaced repetition (Leitner boxes):** each vocab entry carries `box` (1‚Äì5) and `next_due`. Answer right ‚Üí box bumps up and the next review drifts further away; miss it and you return to box 1.
- **`due_words` picker:** calls `_due_indices()` to grab overdue items, sorts by box, then reads the top few so you know what to revise.

```python
while session.active:
    frames = mic.read()
    if vad.is_speech(frames):
        stt.push(frames)
    partial = stt.get_partial()
    if partial:
        agent.ingest_transcript(partial)
    if agent.needs_reply():
        text = llm.complete(agent.context)
        audio = tts.speak(text)
        speaker.play(audio)
```

## üìö Documentation

- **[Quick Start Guide](docs/QUICKSTART.md)** - Get running in 5 minutes
- **[Universal Agent Docs](docs/UNIVERSAL_AGENT.md)** - Complete guide and API reference
- **[Example Questions](docs/EXAMPLE_QUESTIONS.md)** - 100+ questions you can ask
- **[Migration Guide](docs/MIGRATION_GUIDE.md)** - Transition from old agents
- **[Transformation Summary](TRANSFORMATION_SUMMARY.md)** - What's new in this repo

## Features mapped to files
- `voice_livekit_agent/livekit_basic_agent.py` ‚Äì minimal LiveKit demo with optional tool toggle; great for smoke tests.
  - Try saying: ‚ÄúWhat‚Äôs the weather?‚Äù (LLM reply); toggle text/audio with `Ctrl+B`.
- `voice_livekit_agent/french_voice_tutor.py` ‚Äì friendly tutor with CEFR levels, roleplays, phrase packs.
  - Commands: ‚Äúset level B1‚Äù, ‚Äúroleplay caf√©‚Äù, ‚Äúadd vocab bonjour; hello‚Äù.
- `voice_livekit_agent/french_voice_tutor_plus.py` ‚Äì everything above plus SRS quizzes, `due_words`, pronunciation checker, auto CSV/JSON exports.
  - Commands: ‚Äúwhat‚Äôs due today?‚Äù, ‚Äústart quiz 8 en2fr‚Äù, ‚Äúgive me a hotel sentence‚Äù, then speak it back.
- `scripts/smoke_test.py` ‚Äì local LLM sanity check (no audio).
- `scripts/srs_daily.py` ‚Äì prints due vocab and shows a macOS notification; wire it into `launchd` for your morning reminder.

## Environment variables
| Key | Default | Required? | Notes |
| --- | --- | --- | --- |
| `OPENAI_BASE_URL` | `http://127.0.0.1:11434/v1` | ‚úÖ | Points the OpenAI client at Ollama. |
| `OPENAI_API_KEY` | `ollama` | ‚úÖ | Ollama ignores the value but the header must exist. |
| `LLM_MODEL` | `llama3.2:3b` | ‚úÖ | Any Ollama model slug; smaller models = faster. |
| `LLM_TEMPERATURE` | `0.6` | Optional | Lower for more deterministic responses. |
| `BASIC_TEST` | `true` | ‚úÖ | When true, agents run a smoke test then exit. Flip to false for full voice mode. |
| `WARMUP_LLM` | `true` | Optional | Keeps the first real reply quick. |
| `WARMUP_TIMEOUT` | `60` | Optional | Timeout for the warm-up request. |
| `USE_TOOLS` | `false` | Optional | Turn on to let LLM call `@function_tool`s (recommended for tutors). |
| `DEEPGRAM_API_KEY` | ‚Äî | Required when `BASIC_TEST=false` | Enables Deepgram STT. |
| `DEEPGRAM_MODEL` | `nova-2` | Optional | Switch to lighter models if bandwidth is tight. |
| `OPENAI_TTS_API_KEY` | ‚Äî | Required when `BASIC_TEST=false` | Powers OpenAI TTS. |
| `TTS_VOICE` | `alloy` | Optional | Change to any supported voice. |
| `TTS_PROVIDER` | `deepgram` or `openai` | Optional | Swaps TTS backend; defaults to OpenAI. |
| `DEEPGRAM_TTS_MODEL` | `aura-2-andromeda-en` | Optional | Used when `TTS_PROVIDER=deepgram`. |
| `OPENAI_TTS_BASE_URL` | `https://api.openai.com/v1` | Optional | Forces TTS away from the Ollama base URL. |
| `SRS_STATE_FILE` | `french_progress_plus.json` | Optional | Where tutor progress is stored. |
| `SRS_EXPORT_DIR` | `~/.french_tutor/exports` | Optional | Export directory for CSV/JSON snapshots. |
| `SRS_AUTO_EXPORT` | `true` | Optional | Auto-export after vocab or quiz updates. |
| `SRS_EXPORT_FORMAT` | `csv` | Optional | `csv`, `json`, or `both`. |

**Never commit `.env`.** Use the supplied `.gitignore`, or in GitHub Codespaces run `gh secret set` to store keys securely.

## Run modes & controls
- `BASIC_TEST=true` ‚Üí agents print ‚ÄúLLM CHECK OK ‚Ä¶‚Äù and exit; flip to `false` for the full voice loop.
- `Ctrl+B` in LiveKit console toggles Text ‚Üî Audio so you can demo with or without speakers.
- Leave `WARMUP_LLM=true`; it preloads the Ollama model and slashes first-token latency.
- The `scripts/smoke_test.py` helper is perfect for CI or quick checks‚Äîno Deepgram/OpenAI keys needed.

## Learning features (tutor builds)
- **Roleplay** topics: caf√©, travel, hotel. Ask: ‚Äúroleplay travel‚Äù and the tutor acts the scene with a phrase bank.
- **Vocab quiz (SRS)**: ‚Äústart quiz 5 en2fr‚Äù launches Leitner-box practice; answer out loud then hear feedback.
- **Due words**: ‚Äúwhat‚Äôs due today?‚Äù reads the top items waiting for review, nudging you to quiz.
- **Pronunciation**: ‚Äúgive me a caf√© sentence‚Äù ‚Üí repeat it; the tutor compares transcripts and offers accent tips.
- **Exports + reminders**: snapshots land in `~/.french_tutor/exports`; schedule `scripts/srs_daily.py` with `launchd` to get a morning toast.

## Troubleshooting
- **Ollama model not found** ‚Üí `ollama pull llama3.2:3b`.
- **Port already in use** ‚Üí `lsof -i :11434` then restart `ollama serve`.
- **First response slow** ‚Üí ensure `WARMUP_LLM=true` or manually run the tutor once before class.
- **Mic muted on macOS** ‚Üí grant Terminal microphone access in System Settings ‚Üí Privacy & Security ‚Üí Microphone.
- **Missing Deepgram or TTS key** ‚Üí errors show `Missing required environment variables`. Add them to `.env`.
- **No audio playback in console** ‚Üí hit `Ctrl+B` to leave text-only mode; confirm your speakers aren‚Äôt muted.
- **Export folder permissions** ‚Üí create it manually: `mkdir -p ~/.french_tutor/exports`.

## Performance & cost tips
- **Model size**: `llama3.2:3b` balances speed and quality. Drop to `llama3.2:1b` for faster replies with simpler language.
- **Sentence length**: shorter responses cut TTS lag. Encourage the tutor to keep answers to one or two sentences.
- **Streaming chunk size**: leave defaults; lowering Deepgram buffer can raise costs without big gains.
- **Silences**: long pauses still cost STT time‚Äîkeep conversations brisk.
- **Alternative providers**: swap to `TTS_PROVIDER=deepgram` if OpenAI TTS hits quota.

## Safety, privacy, and educational use
- For educational purposes only‚Äîdon‚Äôt feed personal addresses, exam answers, or real customer data.
- Audio streams go to Deepgram (STT) and your chosen TTS provider; text hits your local Ollama instance.
- `.env` stays local; secrets never touch git. If you demo online, rotate keys afterwards.
- Remind students to anonymise any shared vocab exports.

## For educators
- **45-minute lesson plan**  
  1. Quickstart commands (10 min).  
  2. Diagram the pipeline with the Mermaid charts (10 min).  
  3. Let teams customise prompts or add vocab (15 min).  
  4. Share reflections and latency hacks (10 min).
- **Glossary**  
  - *Token*: a tiny chunk of text the LLM reads/writes.  
  - *Latency*: time between speaking and hearing the reply.  
  - *Streaming*: sending data in pieces instead of one big blob.  
  - *VAD*: Voice Activity Detection‚Äîspots when someone‚Äôs talking.  
  - *STT*: Speech to Text‚Äîturns audio into words.  
  - *TTS*: Text to Speech‚Äîturns words back into audio.  
  - *Prompt*: the text instruction given to the LLM.  
  - *Tool call*: when the LLM triggers a Python function like `start_quiz`.

## License & credits
Credit to the LiveKit Agents team for the SDK, Deepgram for STT, Silero for VAD, Ollama for on-device LLMs, and OpenAI for TTS. Project organised and documented by Sam Kalaliya.
